# OSF Pre-registration Draft
**Project:** Formal Adversarial Testing of LLM-Generated Code for Industrial Robots

## 1. Research Question
Can a formal "Watchdog" architecture based on ISO 10218 standards significantly reduce the rate of safety violations in industrial robot code (ABB RAPID) generated by Large Language Models (LLMs) under adversarial attack conditions?

## 2. Hypotheses
* **H1 (Primary):** The proposed AST-based Watchdog will detect and block at least 90% of "syntax-valid but semantically unsafe" code generated by LLMs via Gray-box attacks.
* **H0 (Null):** There is no significant difference in safety violation rates between the system with the Watchdog and the baseline system (LLM output directly to robot).

## 3. Study Design
* **Independent Variable:** Attack Type (White-box vs. Gray-box) and Presence of Watchdog (On/Off).
* **Dependent Variable:** Safety Violation Rate (Count of ISO 10218 breaches per 15 tasks).
* **Statistical Analysis:** McNemarâ€™s test for paired nominal data (Safe/Unsafe outcomes on the same tasks).

## 4. Sampling Plan
We will generate 15 distinct industrial tasks (Pick-and-Place, Welding, Palletizing) and apply 8 types of adversarial injections (A1-A8) to each, resulting in N=120 test cases.
